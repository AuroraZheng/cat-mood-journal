{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edaa7b3d-f3e6-4db3-b946-4868d925b110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è§£å‹åçš„å†…å®¹ï¼š ['README.roboflow.txt', 'valid', 'README.dataset.txt', 'train_by_emotion', '.DS_Store', 'test', 'valid_by_emotion', 'train']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"/Users/aurorazheng/documents/cat-mood-journal/cat.v10i.coco.zip\"\n",
    "extract_path = \"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset\"\n",
    "\n",
    "# è§£å‹\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "# æŸ¥çœ‹è§£å‹åæ–‡ä»¶\n",
    "print(\"è§£å‹åçš„å†…å®¹ï¼š\", os.listdir(extract_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a08b07-48d0-4ae2-8954-7d1b221413d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, shutil\n",
    "\n",
    "def coco_to_classification(coco_json_path, image_dir, output_dir):\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # ç±»åˆ« id åˆ°åå­—çš„æ˜ å°„\n",
    "    categories = {cat[\"id\"]: cat[\"name\"] for cat in data[\"categories\"]}\n",
    "    # å›¾ç‰‡ id åˆ°æ–‡ä»¶åæ˜ å°„\n",
    "    image_map = {img[\"id\"]: img[\"file_name\"] for img in data[\"images\"]}\n",
    "\n",
    "    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # éå†æ ‡æ³¨\n",
    "    for ann in data[\"annotations\"]:\n",
    "        img_id = ann[\"image_id\"]\n",
    "        cat_id = ann[\"category_id\"]\n",
    "\n",
    "        label = categories[cat_id]\n",
    "        file_name = image_map[img_id]\n",
    "\n",
    "        src = os.path.join(image_dir, file_name)\n",
    "        dst_dir = os.path.join(output_dir, label)\n",
    "        dst = os.path.join(dst_dir, file_name)\n",
    "\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "# æ‰§è¡Œè½¬æ¢\n",
    "coco_to_classification(\n",
    "    coco_json_path=\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train/_annotations.coco.json\",\n",
    "    image_dir=\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train/images\",\n",
    "    output_dir=\"/Users/aurorazheng/documents/cat-mood-journal/cat_classification/train\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3336fe61-cb31-4f89-81ff-aabc5912d0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å›¾ç‰‡å·²æŒ‰æƒ…ç»ªåˆ†ç±»æ•´ç†å®Œæ¯•\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "# è·¯å¾„è®¾ç½®\n",
    "annotation_path = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train/_annotations.coco.json'\n",
    "images_dir = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train'  # æ‰€æœ‰å›¾ç‰‡æ‰€åœ¨çš„è·¯å¾„\n",
    "output_dir = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train_by_emotion'  # è¾“å‡ºç›®æ ‡æ–‡ä»¶å¤¹\n",
    "\n",
    "# åŠ è½½æ ‡æ³¨\n",
    "with open(annotation_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# å»ºç«‹ id -> ç±»åˆ«å çš„æ˜ å°„\n",
    "category_map = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "\n",
    "# æ„å»º image_id -> file_name æ˜ å°„\n",
    "image_id_to_file = {img['id']: img['file_name'] for img in coco_data['images']}\n",
    "\n",
    "# æ„å»º image_id -> emotion_id æ˜ å°„ï¼ˆåªå–ç¬¬ä¸€ä¸ªæ ‡æ³¨çš„ç±»åˆ«ï¼‰\n",
    "image_id_to_emotion = {}\n",
    "for ann in coco_data['annotations']:\n",
    "    img_id = ann['image_id']\n",
    "    cat_id = ann['category_id']\n",
    "    # å¦‚æœä¸€å¼ å›¾è¢«æ‰“äº†å¤šä¸ªæ ‡ç­¾ï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ªæ ‡ç­¾ï¼ˆå¯è‡ªè¡Œä¿®æ”¹é€»è¾‘ï¼‰\n",
    "    if img_id not in image_id_to_emotion:\n",
    "        image_id_to_emotion[img_id] = cat_id\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºå­æ–‡ä»¶å¤¹\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for cat_id, cat_name in category_map.items():\n",
    "    os.makedirs(os.path.join(output_dir, cat_name), exist_ok=True)\n",
    "\n",
    "# å¤åˆ¶å›¾ç‰‡åˆ°å¯¹åº”å­æ–‡ä»¶å¤¹\n",
    "for img_id, cat_id in image_id_to_emotion.items():\n",
    "    filename = image_id_to_file[img_id]\n",
    "    src_path = os.path.join(images_dir, filename)\n",
    "    dst_path = os.path.join(output_dir, category_map[cat_id], filename)\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    else:\n",
    "        print(f\"â—ï¸Image not found: {src_path}\")\n",
    "\n",
    "print(\"âœ… å›¾ç‰‡å·²æŒ‰æƒ…ç»ªåˆ†ç±»æ•´ç†å®Œæ¯•\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e361bd4-3421-4aeb-811d-0ed8880de92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å›¾ç‰‡å·²æŒ‰æƒ…ç»ªåˆ†ç±»æ•´ç†å®Œæ¯•\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "# è·¯å¾„è®¾ç½®\n",
    "annotation_path = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train/_annotations.coco.json'\n",
    "images_dir = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train'  # æ‰€æœ‰å›¾ç‰‡æ‰€åœ¨çš„è·¯å¾„\n",
    "output_dir = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train_by_emotion'  # è¾“å‡ºç›®æ ‡æ–‡ä»¶å¤¹\n",
    "\n",
    "# åŠ è½½æ ‡æ³¨\n",
    "with open(annotation_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# å»ºç«‹ id -> ç±»åˆ«å çš„æ˜ å°„\n",
    "category_map = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "\n",
    "# æ„å»º image_id -> file_name æ˜ å°„\n",
    "image_id_to_file = {img['id']: img['file_name'] for img in coco_data['images']}\n",
    "\n",
    "# æ„å»º image_id -> emotion_id æ˜ å°„ï¼ˆåªå–ç¬¬ä¸€ä¸ªæ ‡æ³¨çš„ç±»åˆ«ï¼‰\n",
    "image_id_to_emotion = {}\n",
    "for ann in coco_data['annotations']:\n",
    "    img_id = ann['image_id']\n",
    "    cat_id = ann['category_id']\n",
    "    # å¦‚æœä¸€å¼ å›¾è¢«æ‰“äº†å¤šä¸ªæ ‡ç­¾ï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ªæ ‡ç­¾ï¼ˆå¯è‡ªè¡Œä¿®æ”¹é€»è¾‘ï¼‰\n",
    "    if img_id not in image_id_to_emotion:\n",
    "        image_id_to_emotion[img_id] = cat_id\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºå­æ–‡ä»¶å¤¹\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for cat_id, cat_name in category_map.items():\n",
    "    os.makedirs(os.path.join(output_dir, cat_name), exist_ok=True)\n",
    "\n",
    "# å¤åˆ¶å›¾ç‰‡åˆ°å¯¹åº”å­æ–‡ä»¶å¤¹\n",
    "for img_id, cat_id in image_id_to_emotion.items():\n",
    "    filename = image_id_to_file[img_id]\n",
    "    src_path = os.path.join(images_dir, filename)\n",
    "    dst_path = os.path.join(output_dir, category_map[cat_id], filename)\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    else:\n",
    "        print(f\"â—ï¸Image not found: {src_path}\")\n",
    "\n",
    "print(\"âœ… å›¾ç‰‡å·²æŒ‰æƒ…ç»ªåˆ†ç±»æ•´ç†å®Œæ¯•\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5415b7-d2ad-44dc-909f-56e12ced07e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… validå›¾ç‰‡å·²æŒ‰æƒ…ç»ªåˆ†ç±»æ•´ç†å®Œæ¯•\n"
     ]
    }
   ],
   "source": [
    "# è·¯å¾„è®¾ç½®\n",
    "annotation_path = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/valid/_annotations.coco.json'\n",
    "images_dir = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/valid'  # æ‰€æœ‰å›¾ç‰‡æ‰€åœ¨çš„è·¯å¾„\n",
    "output_dir = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/valid_by_emotion'  # è¾“å‡ºç›®æ ‡æ–‡ä»¶å¤¹\n",
    "\n",
    "# åŠ è½½æ ‡æ³¨\n",
    "with open(annotation_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# å»ºç«‹ id -> ç±»åˆ«å çš„æ˜ å°„\n",
    "category_map = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "\n",
    "# æ„å»º image_id -> file_name æ˜ å°„\n",
    "image_id_to_file = {img['id']: img['file_name'] for img in coco_data['images']}\n",
    "\n",
    "# æ„å»º image_id -> emotion_id æ˜ å°„ï¼ˆåªå–ç¬¬ä¸€ä¸ªæ ‡æ³¨çš„ç±»åˆ«ï¼‰\n",
    "image_id_to_emotion = {}\n",
    "for ann in coco_data['annotations']:\n",
    "    img_id = ann['image_id']\n",
    "    cat_id = ann['category_id']\n",
    "    # å¦‚æœä¸€å¼ å›¾è¢«æ‰“äº†å¤šä¸ªæ ‡ç­¾ï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ªæ ‡ç­¾ï¼ˆå¯è‡ªè¡Œä¿®æ”¹é€»è¾‘ï¼‰\n",
    "    if img_id not in image_id_to_emotion:\n",
    "        image_id_to_emotion[img_id] = cat_id\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºå­æ–‡ä»¶å¤¹\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for cat_id, cat_name in category_map.items():\n",
    "    os.makedirs(os.path.join(output_dir, cat_name), exist_ok=True)\n",
    "\n",
    "# å¤åˆ¶å›¾ç‰‡åˆ°å¯¹åº”å­æ–‡ä»¶å¤¹\n",
    "for img_id, cat_id in image_id_to_emotion.items():\n",
    "    filename = image_id_to_file[img_id]\n",
    "    src_path = os.path.join(images_dir, filename)\n",
    "    dst_path = os.path.join(output_dir, category_map[cat_id], filename)\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    else:\n",
    "        print(f\"â—ï¸Image not found: {src_path}\")\n",
    "\n",
    "print(\"âœ… validå›¾ç‰‡å·²æŒ‰æƒ…ç»ªåˆ†ç±»æ•´ç†å®Œæ¯•\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e440042f-850e-4341-bed0-f6f7db9c8e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "280960f2-7da9-42b2-b936-93f1d0692cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb6aefbb-167d-4ba7-9fcc-b5fa3ccabe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pillow in /Users/aurorazheng/Library/Python/3.9/lib/python/site-packages (11.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5486374c-581a-4260-a026-1abd75cdbeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pillow\n",
      "Version: 11.1.0\n",
      "Summary: Python Imaging Library (Fork)\n",
      "Home-page: https://python-pillow.github.io\n",
      "Author: \n",
      "Author-email: \"Jeffrey A. Clark\" <aclark@aclark.net>\n",
      "License: MIT-CMU\n",
      "Location: /Users/aurorazheng/Library/Python/3.9/lib/python/site-packages\n",
      "Requires: \n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b5d63-7914-49b9-8081-d48c0d9f5b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#é‡å¯å°±å¥½äº†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95911373-78bd-4d12-9a37-926f6cb6a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76d2b229-d8a2-4da0-a62c-e57eb10405d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1258 images belonging to 6 classes.\n",
      "Found 312 images belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aurorazheng/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 342ms/step - accuracy: 0.3623 - loss: 1.5324 - val_accuracy: 0.3974 - val_loss: 1.2306\n",
      "Epoch 2/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 351ms/step - accuracy: 0.6407 - loss: 0.9797 - val_accuracy: 0.4840 - val_loss: 1.2271\n",
      "Epoch 3/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 302ms/step - accuracy: 0.7356 - loss: 0.7425 - val_accuracy: 0.5994 - val_loss: 1.0940\n",
      "Epoch 4/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 296ms/step - accuracy: 0.8156 - loss: 0.5487 - val_accuracy: 0.5865 - val_loss: 1.2662\n",
      "Epoch 5/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 323ms/step - accuracy: 0.8565 - loss: 0.4709 - val_accuracy: 0.6090 - val_loss: 1.1636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è®­ç»ƒå®Œæ¯•\n"
     ]
    }
   ],
   "source": [
    "# did not fit js\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.utils import load_img\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# å‚æ•°\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "train_dir = \"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train_by_emotion\"\n",
    "\n",
    "# æ•°æ®å¢å¼º\n",
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# æ„å»ºæ¨¡å‹\n",
    "base_model = MobileNetV2(input_shape=(img_size, img_size, 3), include_top=False, weights=\"imagenet\")\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(train_gen.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_mobilenet.h5\")\n",
    "print(\"âœ… è®­ç»ƒå®Œæ¯•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5012d627-48a7-4a0a-a37c-215728850808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1258 images belonging to 6 classes.\n",
      "Found 312 images belonging to 6 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_0.75_224_no_top.h5\n",
      "\u001b[1m5903360/5903360\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aurorazheng/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 292ms/step - accuracy: 0.3895 - loss: 1.4981 - val_accuracy: 0.6218 - val_loss: 1.1409\n",
      "Epoch 2/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 288ms/step - accuracy: 0.6478 - loss: 0.9378 - val_accuracy: 0.6314 - val_loss: 1.0330\n",
      "Epoch 3/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 284ms/step - accuracy: 0.7377 - loss: 0.7412 - val_accuracy: 0.6538 - val_loss: 1.0348\n",
      "Epoch 4/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 253ms/step - accuracy: 0.7974 - loss: 0.5865 - val_accuracy: 0.6058 - val_loss: 1.0316\n",
      "Epoch 5/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 263ms/step - accuracy: 0.8667 - loss: 0.4619 - val_accuracy: 0.6699 - val_loss: 0.9719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è®­ç»ƒå®Œæ¯•\n"
     ]
    }
   ],
   "source": [
    "#did not fit Uncaught (in promise) _ValueError: Corrupted configuration, expected array for nodeData: [object Object]\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.utils import load_img\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# å‚æ•°\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "train_dir = \"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train_by_emotion\"\n",
    "\n",
    "# æ•°æ®å¢å¼º\n",
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# æ„å»ºæ¨¡å‹\n",
    "base_model = MobileNetV2(input_shape=(img_size, img_size, 3), include_top=False, weights=\"imagenet\", alpha=0.75)\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(train_gen.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_mobilenet75.h5\")\n",
    "print(\"âœ… è®­ç»ƒå®Œæ¯•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e5fb93-f9e3-4eb8-873c-0ef64de7b065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mğŸŒ² Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter \\\n",
    "  --input_format=keras \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_mobilenet75.h5 \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/mobilenet75/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f25fcb8-ced6-464e-90c0-c34713c3ba9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä¿®å¤å®Œæ¯•ï¼Œä¿å­˜ä¸º model_fixed.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/mobilenet75/model.json\", \"r\") as f:\n",
    "    model = json.load(f)\n",
    "\n",
    "# ä¿®å¤ inbound_nodes é”™è¯¯\n",
    "for layer in model[\"modelTopology\"][\"model_config\"][\"config\"][\"layers\"]:\n",
    "    if \"inbound_nodes\" in layer:\n",
    "        if isinstance(layer[\"inbound_nodes\"], dict):  # é”™è¯¯ç±»å‹\n",
    "            # æŠŠå®ƒåŒ…è£¹æˆæ•°ç»„ï¼ˆç¤ºä¾‹ä¿®å¤ï¼‰\n",
    "            layer[\"inbound_nodes\"] = [[[]]]  # ä¸´æ—¶ä¿®å¤\n",
    "            print(f\"âš ï¸ ä¿®å¤äº†å±‚ {layer['name']} çš„ inbound_nodes ä¸ºç©ºåˆ—è¡¨\")\n",
    "\n",
    "# ä¿å­˜æ–° json\n",
    "with open(\"/Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/mobilenet75/model_fixed.json\", \"w\") as f:\n",
    "    json.dump(model, f)\n",
    "\n",
    "print(\"âœ… ä¿®å¤å®Œæ¯•ï¼Œä¿å­˜ä¸º model_fixed.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3da703c-5bce-46ff-aa23-8dc3fa649a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1258 images belonging to 6 classes.\n",
      "Found 312 images belonging to 6 classes.\n",
      "Epoch 1/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 304ms/step - accuracy: 0.3386 - loss: 1.5538 - val_accuracy: 0.5673 - val_loss: 1.2003\n",
      "Epoch 2/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 266ms/step - accuracy: 0.6477 - loss: 0.9579 - val_accuracy: 0.5513 - val_loss: 1.1108\n",
      "Epoch 3/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 317ms/step - accuracy: 0.7532 - loss: 0.7310 - val_accuracy: 0.6218 - val_loss: 1.0633\n",
      "Epoch 4/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 347ms/step - accuracy: 0.7937 - loss: 0.6230 - val_accuracy: 0.6410 - val_loss: 1.0096\n",
      "Epoch 5/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 295ms/step - accuracy: 0.8821 - loss: 0.4457 - val_accuracy: 0.6506 - val_loss: 0.9753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è®­ç»ƒå®Œæ¯•\n"
     ]
    }
   ],
   "source": [
    "# no sequential\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.utils import load_img\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# å‚æ•°\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "train_dir = \"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train_by_emotion\"\n",
    "\n",
    "# æ•°æ®å¢å¼º\n",
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet', alpha=0.75)\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "outputs = Dense(train_gen.num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_mobilenet75n.h5\")\n",
    "print(\"âœ… è®­ç»ƒå®Œæ¯•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1bc6c49-f888-4032-87c0-d1ceefd14208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1258 images belonging to 6 classes.\n",
      "Found 312 images belonging to 6 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_0.35_224_no_top.h5\n",
      "\u001b[1m2019640/2019640\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Epoch 1/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.3396 - loss: 1.6456 - val_accuracy: 0.5256 - val_loss: 1.1749\n",
      "Epoch 2/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 135ms/step - accuracy: 0.6057 - loss: 0.9689 - val_accuracy: 0.5641 - val_loss: 1.1831\n",
      "Epoch 3/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 130ms/step - accuracy: 0.7292 - loss: 0.7708 - val_accuracy: 0.5801 - val_loss: 1.1403\n",
      "Epoch 4/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 135ms/step - accuracy: 0.7887 - loss: 0.6318 - val_accuracy: 0.5737 - val_loss: 1.1239\n",
      "Epoch 5/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 131ms/step - accuracy: 0.8536 - loss: 0.5222 - val_accuracy: 0.6122 - val_loss: 1.1372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è®­ç»ƒå®Œæ¯•\n"
     ]
    }
   ],
   "source": [
    "# no sequential\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.utils import load_img\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# å‚æ•°\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "train_dir = \"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train_by_emotion\"\n",
    "\n",
    "# æ•°æ®å¢å¼º\n",
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet', alpha=0.35)\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "outputs = Dense(train_gen.num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_mobilenet25.h5\")\n",
    "print(\"âœ… è®­ç»ƒå®Œæ¯•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cabbe949-9c9d-4286-a089-51ee73dfe252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mğŸŒ² Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter \\\n",
    "  --input_format=keras \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_mobilenet25.h5 \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/mobilenet25/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6da22468-1c3e-444c-bef1-7339bb718a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1258 images belonging to 6 classes.\n",
      "Found 312 images belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aurorazheng/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1s/step - accuracy: 0.2754 - loss: 1.6587 - val_accuracy: 0.3429 - val_loss: 1.5503\n",
      "Epoch 2/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.3176 - loss: 1.5685 - val_accuracy: 0.3429 - val_loss: 1.5933\n",
      "Epoch 3/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - accuracy: 0.3638 - loss: 1.5240 - val_accuracy: 0.2564 - val_loss: 1.5813\n",
      "Epoch 4/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 2s/step - accuracy: 0.3217 - loss: 1.5572 - val_accuracy: 0.3654 - val_loss: 1.5072\n",
      "Epoch 5/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 2s/step - accuracy: 0.3753 - loss: 1.4874 - val_accuracy: 0.3397 - val_loss: 1.5167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    }
   ],
   "source": [
    "# did not fit js\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.utils import load_img\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "train_dir = \"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train_by_emotion\"\n",
    "\n",
    "# æ•°æ®å¢å¼º\n",
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# æ„å»ºæ¨¡å‹\n",
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "base_model.trainable = False  # å†»ç»“é¢„è®­ç»ƒå±‚\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(train_gen.num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_tensor, outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "# å¯¼å‡ºä¸º tfjs\n",
    "tfjs.converters.save_keras_model(model, '/Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/mobilenet')\n",
    "# model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_resnet50.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3161fb5-9f72-4ed3-9ff3-cff88ce28bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1258 images belonging to 6 classes.\n",
      "Found 312 images belonging to 6 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cat_mood_cnn\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"cat_mood_cnn\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gap (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input (\u001b[38;5;33mInputLayer\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1 (\u001b[38;5;33mConv2D\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m16\u001b[0m)   â”‚           \u001b[38;5;34m448\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ pool1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m16\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2 (\u001b[38;5;33mConv2D\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚         \u001b[38;5;34m4,640\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gap (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense1 (\u001b[38;5;33mDense\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m2,112\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output (\u001b[38;5;33mDense\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              â”‚           \u001b[38;5;34m390\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,590</span> (29.65 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,590\u001b[0m (29.65 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,590</span> (29.65 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,590\u001b[0m (29.65 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@tensorflow_tfjs.js?v=f2012098:21559 Uncaught (in promise) _ValueError: Provided weight data has no target variable: Sequential0/con0/kernel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# å‚æ•°\n",
    "img_size = 128\n",
    "batch_size = 32\n",
    "train_dir = \"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train_by_emotion\"\n",
    "\n",
    "# æ•°æ®å¢å¼º\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense\n",
    "\n",
    "inputs = Input(shape=(128, 128, 3), name='input')\n",
    "x = Conv2D(16, (3, 3), activation='relu', name='conv1')(inputs)\n",
    "x = MaxPooling2D(name='pool1')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', name='conv2')(x)\n",
    "x = GlobalAveragePooling2D(name='gap')(x)\n",
    "x = Dense(64, activation='relu', name='dense1')(x)\n",
    "outputs = Dense(6, activation='softmax', name='output')(x)\n",
    "\n",
    "model = Model(inputs, outputs, name='cat_mood_cnn')\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7adbdaee-b2f8-4058-9379-59b8d65c0df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aurorazheng/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 124ms/step - accuracy: 0.2983 - loss: 1.7391 - val_accuracy: 0.3429 - val_loss: 1.5832\n",
      "Epoch 2/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 102ms/step - accuracy: 0.3440 - loss: 1.5630 - val_accuracy: 0.3429 - val_loss: 1.5557\n",
      "Epoch 3/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 102ms/step - accuracy: 0.3218 - loss: 1.5670 - val_accuracy: 0.3429 - val_loss: 1.5552\n",
      "Epoch 4/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.3444 - loss: 1.5417 - val_accuracy: 0.3429 - val_loss: 1.5482\n",
      "Epoch 5/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - accuracy: 0.3230 - loss: 1.5578 - val_accuracy: 0.3429 - val_loss: 1.5446\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_mood_model.keras\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd8c4994-07e0-4cbd-820a-8390df263c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_mood_model.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "000bca79-209c-41f0-bc1c-d8887cdf2280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cat_mood_cnn\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"cat_mood_cnn\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gap (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input (\u001b[38;5;33mInputLayer\u001b[0m)              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1 (\u001b[38;5;33mConv2D\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m16\u001b[0m)   â”‚           \u001b[38;5;34m448\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ pool1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m16\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2 (\u001b[38;5;33mConv2D\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚         \u001b[38;5;34m4,640\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gap (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense1 (\u001b[38;5;33mDense\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m2,112\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output (\u001b[38;5;33mDense\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              â”‚           \u001b[38;5;34m390\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,592</span> (29.66 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,592\u001b[0m (29.66 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,590</span> (29.65 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,590\u001b[0m (29.65 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_mood_model.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a4aa019-7b48-4f96-b22b-850c10f7e2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mğŸŒ² Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter \\\n",
    "  --input_format keras \\\n",
    "  --output_format=tfjs_layers_model \\\n",
    "  --strip_debug_ops true \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_mood_model.h5 \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/cnn1/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "785d8890-b78a-454f-af7c-aa91b2db811c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… inbound_nodes ä¿®å¤å®Œæˆï¼Œä¿å­˜ä¸º model_fixed.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/cnn1/model.json\", \"r\") as f:\n",
    "    model_data = json.load(f)\n",
    "\n",
    "for layer in model_data[\"modelTopology\"][\"model_config\"][\"config\"][\"layers\"]:\n",
    "    if \"inbound_nodes\" in layer:\n",
    "        nodes = layer[\"inbound_nodes\"]\n",
    "        if isinstance(nodes, list) and len(nodes) > 0 and isinstance(nodes[0], dict):\n",
    "            # â— å¦‚æœæ˜¯å¯¹è±¡ï¼ˆé”™è¯¯æ ¼å¼ï¼‰ï¼ŒåŒ…ä¸¤å±‚ä¸­æ‹¬å·\n",
    "            layer[\"inbound_nodes\"] = [[nodes]]\n",
    "\n",
    "with open(\"/Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/cnn1/model_fixed.json\", \"w\") as f:\n",
    "    json.dump(model_data, f, indent=2)\n",
    "\n",
    "print(\"âœ… inbound_nodes ä¿®å¤å®Œæˆï¼Œä¿å­˜ä¸º model_fixed.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38360a9b-fbdc-4594-8793-1366a50ba95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, Input\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "input_tensor = Input(shape=(224, 224, 3), name=\"input\")\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', name=\"con0\")(input_tensor)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(32, activation='relu', name=\"dense1\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output = layers.Dense(6, activation='softmax', name=\"dense2\")(x)\n",
    "\n",
    "model = models.Model(inputs=input_tensor, outputs=output)\n",
    "\n",
    "# å¯¼å‡ºä¸º tfjs\n",
    "tfjs.converters.save_keras_model(model, '/Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/mobilenet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0848f48-e225-48c4-a435-0ff2d4ebed32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 128, 128, 3], 'dtype': 'float32', 'keras_history': ['input', 0, 0]}}], 'kwargs': {}}\n",
      "{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 126, 126, 32], 'dtype': 'float32', 'keras_history': ['con0', 0, 0]}}], 'kwargs': {}}\n",
      "{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 63, 63, 32], 'dtype': 'float32', 'keras_history': ['max_pooling2d_9', 0, 0]}}], 'kwargs': {}}\n",
      "{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 127008], 'dtype': 'float32', 'keras_history': ['flatten_5', 0, 0]}}], 'kwargs': {}}\n",
      "{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 32], 'dtype': 'float32', 'keras_history': ['dense1', 0, 0]}}], 'kwargs': {'training': False}}\n",
      "{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 32], 'dtype': 'float32', 'keras_history': ['dropout_5', 0, 0]}}], 'kwargs': {}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/model/model.json\", \"r\") as f:\n",
    "    model = json.load(f)\n",
    "\n",
    "for layer in model[\"modelTopology\"][\"model_config\"][\"config\"][\"layers\"]:\n",
    "    nodes = layer.get(\"inbound_nodes\", [])\n",
    "    if nodes and isinstance(nodes[0], dict):  # Keras 3.x é£æ ¼\n",
    "        new_nodes = []\n",
    "        for node in nodes:\n",
    "            print(node)\n",
    "            if isinstance(node[\"args\"][0], dict):\n",
    "              keras_hist = node[\"args\"][0][\"config\"][\"keras_history\"]\n",
    "              new_nodes.append([[keras_hist[0], keras_hist[1], keras_hist[2], {}]])\n",
    "            else:\n",
    "              keras_hist = node[\"args\"][0][0][\"config\"][\"keras_history\"]\n",
    "              new_nodes.append([[[keras_hist[0], keras_hist[1], keras_hist[2], {}]]])\n",
    "\n",
    "        layer[\"inbound_nodes\"] = new_nodes\n",
    "\n",
    "with open(\"/Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/model/model_fixed.json\", \"w\") as f:\n",
    "    json.dump(model, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a777a77a-8de6-4d49-951f-0b8797131453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mğŸŒ² Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter \\\n",
    "  --input_format=keras \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_cnn.h5 \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/cnn/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d24acc0-7c2c-479f-a613-9c710783d23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/_dbm1zlx12l07dd83yhm_1d00000gn/T/ipykernel_21918/249831199.py:28: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(input_tensor=input_tensor, include_top=False, weights=\"imagenet\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 359ms/step - accuracy: 0.3052 - loss: 1.6836 - val_accuracy: 0.4872 - val_loss: 1.2432\n",
      "Epoch 2/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 332ms/step - accuracy: 0.6088 - loss: 1.0428 - val_accuracy: 0.5833 - val_loss: 1.1117\n",
      "Epoch 3/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 360ms/step - accuracy: 0.7063 - loss: 0.8044 - val_accuracy: 0.5833 - val_loss: 1.1433\n",
      "Epoch 4/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 310ms/step - accuracy: 0.7574 - loss: 0.6923 - val_accuracy: 0.5833 - val_loss: 1.1418\n",
      "Epoch 5/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 310ms/step - accuracy: 0.8438 - loss: 0.5198 - val_accuracy: 0.6186 - val_loss: 1.0926\n",
      "âœ… è®­ç»ƒå®Œæ¯•\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras import layers, models, Input\n",
    "# img_size = 224  # å‡è®¾ä½ ç”¨çš„æ˜¯è¿™ä¸ªå¤§å°\n",
    "\n",
    "# # Functional API æ„å»ºæ¨¡å‹\n",
    "# inputs = Input(shape=(img_size, img_size, 3))\n",
    "# base_model = MobileNetV2(input_tensor=inputs, include_top=False, weights='imagenet')\n",
    "# base_model.trainable = True\n",
    "\n",
    "# x = base_model.output\n",
    "# x = layers.GlobalAveragePooling2D()(x)\n",
    "# x = layers.Dense(128, activation='relu')(x)\n",
    "# outputs = layers.Dense(train_gen.num_classes, activation='softmax')(x)\n",
    "\n",
    "# model = models.Model(inputs, outputs)\n",
    "\n",
    "# # ç¼–è¯‘å’Œè®­ç»ƒ\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "\n",
    "# # ä¿å­˜æ¨¡å‹\n",
    "# model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_mobilenet1.h5\")\n",
    "\n",
    "\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "# æ„å»ºæ¨¡å‹\n",
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "base_model = MobileNetV2(input_tensor=input_tensor, include_top=False, weights=\"imagenet\")\n",
    "base_model.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "output = layers.Dense(train_gen.num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_tensor, outputs=output)\n",
    "\n",
    "# ç¼–è¯‘ & è®­ç»ƒ\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "\n",
    "# ä¿å­˜ä¸º Keras åŸç”Ÿæ ¼å¼ï¼ˆæ¨èï¼‰\n",
    "model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_model.keras\")\n",
    "print(\"âœ… è®­ç»ƒå®Œæ¯•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18b2d9-61c5-439b-a812-6875d636d35f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1266f205-9b66-4e52-86f4-cecfa1322fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è®­ç»ƒå®Œæ¯•\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_model.h5\")\n",
    "print(\"âœ… è®­ç»ƒå®Œæ¯•\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f5c95e0-9b5b-4e60-b076-1e94faea97af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade tensorflowjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33ed42c0-b0fb-493e-84c9-d2cd42031591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mğŸŒ² Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter \\\n",
    "  --input_format=keras \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_model.h5 \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d1ba38e-0ae6-4449-87fd-204882e57a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ mobilenetv2_1.00_224            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ mobilenetv2_1.00_224            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     â”‚     \u001b[38;5;34m2,257,984\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)                    â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚       \u001b[38;5;34m163,968\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              â”‚           \u001b[38;5;34m774\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,422,728</span> (9.24 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,422,728\u001b[0m (9.24 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,742</span> (643.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m164,742\u001b[0m (643.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_mobilenet.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851da202-1896-4ce2-abfc-89fcdd2742b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
