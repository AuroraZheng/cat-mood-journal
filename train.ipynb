{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edaa7b3d-f3e6-4db3-b946-4868d925b110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解压后的内容： ['README.roboflow.txt', 'valid', 'README.dataset.txt', 'train_by_emotion', '.DS_Store', 'test', 'valid_by_emotion', 'train']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"/Users/aurorazheng/documents/cat-mood-journal/cat.v10i.coco.zip\"\n",
    "extract_path = \"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset\"\n",
    "\n",
    "# 解压\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "# 查看解压后文件\n",
    "print(\"解压后的内容：\", os.listdir(extract_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a08b07-48d0-4ae2-8954-7d1b221413d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, shutil\n",
    "\n",
    "def coco_to_classification(coco_json_path, image_dir, output_dir):\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 类别 id 到名字的映射\n",
    "    categories = {cat[\"id\"]: cat[\"name\"] for cat in data[\"categories\"]}\n",
    "    # 图片 id 到文件名映射\n",
    "    image_map = {img[\"id\"]: img[\"file_name\"] for img in data[\"images\"]}\n",
    "\n",
    "    # 创建输出文件夹\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 遍历标注\n",
    "    for ann in data[\"annotations\"]:\n",
    "        img_id = ann[\"image_id\"]\n",
    "        cat_id = ann[\"category_id\"]\n",
    "\n",
    "        label = categories[cat_id]\n",
    "        file_name = image_map[img_id]\n",
    "\n",
    "        src = os.path.join(image_dir, file_name)\n",
    "        dst_dir = os.path.join(output_dir, label)\n",
    "        dst = os.path.join(dst_dir, file_name)\n",
    "\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "# 执行转换\n",
    "coco_to_classification(\n",
    "    coco_json_path=\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train/_annotations.coco.json\",\n",
    "    image_dir=\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train/images\",\n",
    "    output_dir=\"/Users/aurorazheng/documents/cat-mood-journal/cat_classification/train\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3336fe61-cb31-4f89-81ff-aabc5912d0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 图片已按情绪分类整理完毕\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "# 路径设置\n",
    "annotation_path = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train/_annotations.coco.json'\n",
    "images_dir = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train'  # 所有图片所在的路径\n",
    "output_dir = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train_by_emotion'  # 输出目标文件夹\n",
    "\n",
    "# 加载标注\n",
    "with open(annotation_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# 建立 id -> 类别名 的映射\n",
    "category_map = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "\n",
    "# 构建 image_id -> file_name 映射\n",
    "image_id_to_file = {img['id']: img['file_name'] for img in coco_data['images']}\n",
    "\n",
    "# 构建 image_id -> emotion_id 映射（只取第一个标注的类别）\n",
    "image_id_to_emotion = {}\n",
    "for ann in coco_data['annotations']:\n",
    "    img_id = ann['image_id']\n",
    "    cat_id = ann['category_id']\n",
    "    # 如果一张图被打了多个标签，只保留第一个标签（可自行修改逻辑）\n",
    "    if img_id not in image_id_to_emotion:\n",
    "        image_id_to_emotion[img_id] = cat_id\n",
    "\n",
    "# 创建输出子文件夹\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for cat_id, cat_name in category_map.items():\n",
    "    os.makedirs(os.path.join(output_dir, cat_name), exist_ok=True)\n",
    "\n",
    "# 复制图片到对应子文件夹\n",
    "for img_id, cat_id in image_id_to_emotion.items():\n",
    "    filename = image_id_to_file[img_id]\n",
    "    src_path = os.path.join(images_dir, filename)\n",
    "    dst_path = os.path.join(output_dir, category_map[cat_id], filename)\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    else:\n",
    "        print(f\"❗️Image not found: {src_path}\")\n",
    "\n",
    "print(\"✅ 图片已按情绪分类整理完毕\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e361bd4-3421-4aeb-811d-0ed8880de92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 图片已按情绪分类整理完毕\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "# 路径设置\n",
    "annotation_path = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train/_annotations.coco.json'\n",
    "images_dir = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train'  # 所有图片所在的路径\n",
    "output_dir = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train_by_emotion'  # 输出目标文件夹\n",
    "\n",
    "# 加载标注\n",
    "with open(annotation_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# 建立 id -> 类别名 的映射\n",
    "category_map = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "\n",
    "# 构建 image_id -> file_name 映射\n",
    "image_id_to_file = {img['id']: img['file_name'] for img in coco_data['images']}\n",
    "\n",
    "# 构建 image_id -> emotion_id 映射（只取第一个标注的类别）\n",
    "image_id_to_emotion = {}\n",
    "for ann in coco_data['annotations']:\n",
    "    img_id = ann['image_id']\n",
    "    cat_id = ann['category_id']\n",
    "    # 如果一张图被打了多个标签，只保留第一个标签（可自行修改逻辑）\n",
    "    if img_id not in image_id_to_emotion:\n",
    "        image_id_to_emotion[img_id] = cat_id\n",
    "\n",
    "# 创建输出子文件夹\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for cat_id, cat_name in category_map.items():\n",
    "    os.makedirs(os.path.join(output_dir, cat_name), exist_ok=True)\n",
    "\n",
    "# 复制图片到对应子文件夹\n",
    "for img_id, cat_id in image_id_to_emotion.items():\n",
    "    filename = image_id_to_file[img_id]\n",
    "    src_path = os.path.join(images_dir, filename)\n",
    "    dst_path = os.path.join(output_dir, category_map[cat_id], filename)\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    else:\n",
    "        print(f\"❗️Image not found: {src_path}\")\n",
    "\n",
    "print(\"✅ 图片已按情绪分类整理完毕\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5415b7-d2ad-44dc-909f-56e12ced07e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ valid图片已按情绪分类整理完毕\n"
     ]
    }
   ],
   "source": [
    "# 路径设置\n",
    "annotation_path = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/valid/_annotations.coco.json'\n",
    "images_dir = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/valid'  # 所有图片所在的路径\n",
    "output_dir = '/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/valid_by_emotion'  # 输出目标文件夹\n",
    "\n",
    "# 加载标注\n",
    "with open(annotation_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# 建立 id -> 类别名 的映射\n",
    "category_map = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "\n",
    "# 构建 image_id -> file_name 映射\n",
    "image_id_to_file = {img['id']: img['file_name'] for img in coco_data['images']}\n",
    "\n",
    "# 构建 image_id -> emotion_id 映射（只取第一个标注的类别）\n",
    "image_id_to_emotion = {}\n",
    "for ann in coco_data['annotations']:\n",
    "    img_id = ann['image_id']\n",
    "    cat_id = ann['category_id']\n",
    "    # 如果一张图被打了多个标签，只保留第一个标签（可自行修改逻辑）\n",
    "    if img_id not in image_id_to_emotion:\n",
    "        image_id_to_emotion[img_id] = cat_id\n",
    "\n",
    "# 创建输出子文件夹\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for cat_id, cat_name in category_map.items():\n",
    "    os.makedirs(os.path.join(output_dir, cat_name), exist_ok=True)\n",
    "\n",
    "# 复制图片到对应子文件夹\n",
    "for img_id, cat_id in image_id_to_emotion.items():\n",
    "    filename = image_id_to_file[img_id]\n",
    "    src_path = os.path.join(images_dir, filename)\n",
    "    dst_path = os.path.join(output_dir, category_map[cat_id], filename)\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    else:\n",
    "        print(f\"❗️Image not found: {src_path}\")\n",
    "\n",
    "print(\"✅ valid图片已按情绪分类整理完毕\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e440042f-850e-4341-bed0-f6f7db9c8e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "280960f2-7da9-42b2-b936-93f1d0692cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb6aefbb-167d-4ba7-9fcc-b5fa3ccabe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pillow in /Users/aurorazheng/Library/Python/3.9/lib/python/site-packages (11.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5486374c-581a-4260-a026-1abd75cdbeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pillow\n",
      "Version: 11.1.0\n",
      "Summary: Python Imaging Library (Fork)\n",
      "Home-page: https://python-pillow.github.io\n",
      "Author: \n",
      "Author-email: \"Jeffrey A. Clark\" <aclark@aclark.net>\n",
      "License: MIT-CMU\n",
      "Location: /Users/aurorazheng/Library/Python/3.9/lib/python/site-packages\n",
      "Requires: \n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038b5d63-7914-49b9-8081-d48c0d9f5b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#重启就好了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95911373-78bd-4d12-9a37-926f6cb6a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76d2b229-d8a2-4da0-a62c-e57eb10405d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1258 images belonging to 6 classes.\n",
      "Found 312 images belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aurorazheng/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 342ms/step - accuracy: 0.3623 - loss: 1.5324 - val_accuracy: 0.3974 - val_loss: 1.2306\n",
      "Epoch 2/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 351ms/step - accuracy: 0.6407 - loss: 0.9797 - val_accuracy: 0.4840 - val_loss: 1.2271\n",
      "Epoch 3/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 302ms/step - accuracy: 0.7356 - loss: 0.7425 - val_accuracy: 0.5994 - val_loss: 1.0940\n",
      "Epoch 4/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 296ms/step - accuracy: 0.8156 - loss: 0.5487 - val_accuracy: 0.5865 - val_loss: 1.2662\n",
      "Epoch 5/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 323ms/step - accuracy: 0.8565 - loss: 0.4709 - val_accuracy: 0.6090 - val_loss: 1.1636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 训练完毕\n"
     ]
    }
   ],
   "source": [
    "# did not fit js\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.utils import load_img\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# 参数\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "train_dir = \"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train_by_emotion\"\n",
    "\n",
    "# 数据增强\n",
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "base_model = MobileNetV2(input_shape=(img_size, img_size, 3), include_top=False, weights=\"imagenet\")\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(train_gen.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_mobilenet.h5\")\n",
    "print(\"✅ 训练完毕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5012d627-48a7-4a0a-a37c-215728850808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1258 images belonging to 6 classes.\n",
      "Found 312 images belonging to 6 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_0.75_224_no_top.h5\n",
      "\u001b[1m5903360/5903360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aurorazheng/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 292ms/step - accuracy: 0.3895 - loss: 1.4981 - val_accuracy: 0.6218 - val_loss: 1.1409\n",
      "Epoch 2/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 288ms/step - accuracy: 0.6478 - loss: 0.9378 - val_accuracy: 0.6314 - val_loss: 1.0330\n",
      "Epoch 3/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 284ms/step - accuracy: 0.7377 - loss: 0.7412 - val_accuracy: 0.6538 - val_loss: 1.0348\n",
      "Epoch 4/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 253ms/step - accuracy: 0.7974 - loss: 0.5865 - val_accuracy: 0.6058 - val_loss: 1.0316\n",
      "Epoch 5/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 263ms/step - accuracy: 0.8667 - loss: 0.4619 - val_accuracy: 0.6699 - val_loss: 0.9719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 训练完毕\n"
     ]
    }
   ],
   "source": [
    "#did not fit Uncaught (in promise) _ValueError: Corrupted configuration, expected array for nodeData: [object Object]\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.utils import load_img\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# 参数\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "train_dir = \"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train_by_emotion\"\n",
    "\n",
    "# 数据增强\n",
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "base_model = MobileNetV2(input_shape=(img_size, img_size, 3), include_top=False, weights=\"imagenet\", alpha=0.75)\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(train_gen.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_mobilenet75.h5\")\n",
    "print(\"✅ 训练完毕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e5fb93-f9e3-4eb8-873c-0ef64de7b065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m🌲 Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter \\\n",
    "  --input_format=keras \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_mobilenet75.h5 \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/mobilenet75/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f25fcb8-ced6-464e-90c0-c34713c3ba9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 修复完毕，保存为 model_fixed.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/mobilenet75/model.json\", \"r\") as f:\n",
    "    model = json.load(f)\n",
    "\n",
    "# 修复 inbound_nodes 错误\n",
    "for layer in model[\"modelTopology\"][\"model_config\"][\"config\"][\"layers\"]:\n",
    "    if \"inbound_nodes\" in layer:\n",
    "        if isinstance(layer[\"inbound_nodes\"], dict):  # 错误类型\n",
    "            # 把它包裹成数组（示例修复）\n",
    "            layer[\"inbound_nodes\"] = [[[]]]  # 临时修复\n",
    "            print(f\"⚠️ 修复了层 {layer['name']} 的 inbound_nodes 为空列表\")\n",
    "\n",
    "# 保存新 json\n",
    "with open(\"/Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/mobilenet75/model_fixed.json\", \"w\") as f:\n",
    "    json.dump(model, f)\n",
    "\n",
    "print(\"✅ 修复完毕，保存为 model_fixed.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3da703c-5bce-46ff-aa23-8dc3fa649a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1258 images belonging to 6 classes.\n",
      "Found 312 images belonging to 6 classes.\n",
      "Epoch 1/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 304ms/step - accuracy: 0.3386 - loss: 1.5538 - val_accuracy: 0.5673 - val_loss: 1.2003\n",
      "Epoch 2/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 266ms/step - accuracy: 0.6477 - loss: 0.9579 - val_accuracy: 0.5513 - val_loss: 1.1108\n",
      "Epoch 3/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 317ms/step - accuracy: 0.7532 - loss: 0.7310 - val_accuracy: 0.6218 - val_loss: 1.0633\n",
      "Epoch 4/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 347ms/step - accuracy: 0.7937 - loss: 0.6230 - val_accuracy: 0.6410 - val_loss: 1.0096\n",
      "Epoch 5/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 295ms/step - accuracy: 0.8821 - loss: 0.4457 - val_accuracy: 0.6506 - val_loss: 0.9753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 训练完毕\n"
     ]
    }
   ],
   "source": [
    "# no sequential\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.utils import load_img\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# 参数\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "train_dir = \"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train_by_emotion\"\n",
    "\n",
    "# 数据增强\n",
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet', alpha=0.75)\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "outputs = Dense(train_gen.num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_mobilenet75n.h5\")\n",
    "print(\"✅ 训练完毕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1bc6c49-f888-4032-87c0-d1ceefd14208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1258 images belonging to 6 classes.\n",
      "Found 312 images belonging to 6 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_0.35_224_no_top.h5\n",
      "\u001b[1m2019640/2019640\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Epoch 1/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 148ms/step - accuracy: 0.3396 - loss: 1.6456 - val_accuracy: 0.5256 - val_loss: 1.1749\n",
      "Epoch 2/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 135ms/step - accuracy: 0.6057 - loss: 0.9689 - val_accuracy: 0.5641 - val_loss: 1.1831\n",
      "Epoch 3/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 130ms/step - accuracy: 0.7292 - loss: 0.7708 - val_accuracy: 0.5801 - val_loss: 1.1403\n",
      "Epoch 4/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 135ms/step - accuracy: 0.7887 - loss: 0.6318 - val_accuracy: 0.5737 - val_loss: 1.1239\n",
      "Epoch 5/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 131ms/step - accuracy: 0.8536 - loss: 0.5222 - val_accuracy: 0.6122 - val_loss: 1.1372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 训练完毕\n"
     ]
    }
   ],
   "source": [
    "# no sequential\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.utils import load_img\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# 参数\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "train_dir = \"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train_by_emotion\"\n",
    "\n",
    "# 数据增强\n",
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet', alpha=0.35)\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "outputs = Dense(train_gen.num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_mobilenet25.h5\")\n",
    "print(\"✅ 训练完毕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cabbe949-9c9d-4286-a089-51ee73dfe252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m🌲 Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter \\\n",
    "  --input_format=keras \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_mobilenet25.h5 \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/mobilenet25/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6da22468-1c3e-444c-bef1-7339bb718a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1258 images belonging to 6 classes.\n",
      "Found 312 images belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aurorazheng/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1s/step - accuracy: 0.2754 - loss: 1.6587 - val_accuracy: 0.3429 - val_loss: 1.5503\n",
      "Epoch 2/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.3176 - loss: 1.5685 - val_accuracy: 0.3429 - val_loss: 1.5933\n",
      "Epoch 3/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - accuracy: 0.3638 - loss: 1.5240 - val_accuracy: 0.2564 - val_loss: 1.5813\n",
      "Epoch 4/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 2s/step - accuracy: 0.3217 - loss: 1.5572 - val_accuracy: 0.3654 - val_loss: 1.5072\n",
      "Epoch 5/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 2s/step - accuracy: 0.3753 - loss: 1.4874 - val_accuracy: 0.3397 - val_loss: 1.5167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    }
   ],
   "source": [
    "# did not fit js\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "# from tensorflow.keras.utils import load_img\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "train_dir = \"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train_by_emotion\"\n",
    "\n",
    "# 数据增强\n",
    "datagen = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "base_model.trainable = False  # 冻结预训练层\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output = Dense(train_gen.num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_tensor, outputs=output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "# 导出为 tfjs\n",
    "tfjs.converters.save_keras_model(model, '/Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/mobilenet')\n",
    "# model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_resnet50.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3161fb5-9f72-4ed3-9ff3-cff88ce28bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1258 images belonging to 6 classes.\n",
      "Found 312 images belonging to 6 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cat_mood_cnn\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"cat_mood_cnn\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gap (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │           \u001b[38;5;34m448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gap (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense1 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m390\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,590</span> (29.65 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,590\u001b[0m (29.65 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,590</span> (29.65 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,590\u001b[0m (29.65 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@tensorflow_tfjs.js?v=f2012098:21559 Uncaught (in promise) _ValueError: Provided weight data has no target variable: Sequential0/con0/kernel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 参数\n",
    "img_size = 128\n",
    "batch_size = 32\n",
    "train_dir = \"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/train_by_emotion\"\n",
    "\n",
    "# 数据增强\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense\n",
    "\n",
    "inputs = Input(shape=(128, 128, 3), name='input')\n",
    "x = Conv2D(16, (3, 3), activation='relu', name='conv1')(inputs)\n",
    "x = MaxPooling2D(name='pool1')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', name='conv2')(x)\n",
    "x = GlobalAveragePooling2D(name='gap')(x)\n",
    "x = Dense(64, activation='relu', name='dense1')(x)\n",
    "outputs = Dense(6, activation='softmax', name='output')(x)\n",
    "\n",
    "model = Model(inputs, outputs, name='cat_mood_cnn')\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7adbdaee-b2f8-4058-9379-59b8d65c0df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aurorazheng/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 124ms/step - accuracy: 0.2983 - loss: 1.7391 - val_accuracy: 0.3429 - val_loss: 1.5832\n",
      "Epoch 2/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 102ms/step - accuracy: 0.3440 - loss: 1.5630 - val_accuracy: 0.3429 - val_loss: 1.5557\n",
      "Epoch 3/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 102ms/step - accuracy: 0.3218 - loss: 1.5670 - val_accuracy: 0.3429 - val_loss: 1.5552\n",
      "Epoch 4/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.3444 - loss: 1.5417 - val_accuracy: 0.3429 - val_loss: 1.5482\n",
      "Epoch 5/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - accuracy: 0.3230 - loss: 1.5578 - val_accuracy: 0.3429 - val_loss: 1.5446\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_mood_model.keras\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd8c4994-07e0-4cbd-820a-8390df263c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_mood_model.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "000bca79-209c-41f0-bc1c-d8887cdf2280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cat_mood_cnn\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"cat_mood_cnn\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gap (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │           \u001b[38;5;34m448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ pool1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2 (\u001b[38;5;33mConv2D\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m4,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gap (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense1 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m390\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,592</span> (29.66 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,592\u001b[0m (29.66 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,590</span> (29.65 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,590\u001b[0m (29.65 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_mood_model.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a4aa019-7b48-4f96-b22b-850c10f7e2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m🌲 Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter \\\n",
    "  --input_format keras \\\n",
    "  --output_format=tfjs_layers_model \\\n",
    "  --strip_debug_ops true \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_mood_model.h5 \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/cnn1/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "785d8890-b78a-454f-af7c-aa91b2db811c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ inbound_nodes 修复完成，保存为 model_fixed.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/cnn1/model.json\", \"r\") as f:\n",
    "    model_data = json.load(f)\n",
    "\n",
    "for layer in model_data[\"modelTopology\"][\"model_config\"][\"config\"][\"layers\"]:\n",
    "    if \"inbound_nodes\" in layer:\n",
    "        nodes = layer[\"inbound_nodes\"]\n",
    "        if isinstance(nodes, list) and len(nodes) > 0 and isinstance(nodes[0], dict):\n",
    "            # ❗ 如果是对象（错误格式），包两层中括号\n",
    "            layer[\"inbound_nodes\"] = [[nodes]]\n",
    "\n",
    "with open(\"/Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/cnn1/model_fixed.json\", \"w\") as f:\n",
    "    json.dump(model_data, f, indent=2)\n",
    "\n",
    "print(\"✅ inbound_nodes 修复完成，保存为 model_fixed.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38360a9b-fbdc-4594-8793-1366a50ba95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, Input\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "input_tensor = Input(shape=(224, 224, 3), name=\"input\")\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', name=\"con0\")(input_tensor)\n",
    "x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(32, activation='relu', name=\"dense1\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output = layers.Dense(6, activation='softmax', name=\"dense2\")(x)\n",
    "\n",
    "model = models.Model(inputs=input_tensor, outputs=output)\n",
    "\n",
    "# 导出为 tfjs\n",
    "tfjs.converters.save_keras_model(model, '/Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/mobilenet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0848f48-e225-48c4-a435-0ff2d4ebed32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 128, 128, 3], 'dtype': 'float32', 'keras_history': ['input', 0, 0]}}], 'kwargs': {}}\n",
      "{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 126, 126, 32], 'dtype': 'float32', 'keras_history': ['con0', 0, 0]}}], 'kwargs': {}}\n",
      "{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 63, 63, 32], 'dtype': 'float32', 'keras_history': ['max_pooling2d_9', 0, 0]}}], 'kwargs': {}}\n",
      "{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 127008], 'dtype': 'float32', 'keras_history': ['flatten_5', 0, 0]}}], 'kwargs': {}}\n",
      "{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 32], 'dtype': 'float32', 'keras_history': ['dense1', 0, 0]}}], 'kwargs': {'training': False}}\n",
      "{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 32], 'dtype': 'float32', 'keras_history': ['dropout_5', 0, 0]}}], 'kwargs': {}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/model/model.json\", \"r\") as f:\n",
    "    model = json.load(f)\n",
    "\n",
    "for layer in model[\"modelTopology\"][\"model_config\"][\"config\"][\"layers\"]:\n",
    "    nodes = layer.get(\"inbound_nodes\", [])\n",
    "    if nodes and isinstance(nodes[0], dict):  # Keras 3.x 风格\n",
    "        new_nodes = []\n",
    "        for node in nodes:\n",
    "            print(node)\n",
    "            if isinstance(node[\"args\"][0], dict):\n",
    "              keras_hist = node[\"args\"][0][\"config\"][\"keras_history\"]\n",
    "              new_nodes.append([[keras_hist[0], keras_hist[1], keras_hist[2], {}]])\n",
    "            else:\n",
    "              keras_hist = node[\"args\"][0][0][\"config\"][\"keras_history\"]\n",
    "              new_nodes.append([[[keras_hist[0], keras_hist[1], keras_hist[2], {}]]])\n",
    "\n",
    "        layer[\"inbound_nodes\"] = new_nodes\n",
    "\n",
    "with open(\"/Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/model/model_fixed.json\", \"w\") as f:\n",
    "    json.dump(model, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a777a77a-8de6-4d49-951f-0b8797131453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m🌲 Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter \\\n",
    "  --input_format=keras \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_cnn.h5 \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/cnn/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d24acc0-7c2c-479f-a613-9c710783d23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/_dbm1zlx12l07dd83yhm_1d00000gn/T/ipykernel_21918/249831199.py:28: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(input_tensor=input_tensor, include_top=False, weights=\"imagenet\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 359ms/step - accuracy: 0.3052 - loss: 1.6836 - val_accuracy: 0.4872 - val_loss: 1.2432\n",
      "Epoch 2/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 332ms/step - accuracy: 0.6088 - loss: 1.0428 - val_accuracy: 0.5833 - val_loss: 1.1117\n",
      "Epoch 3/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 360ms/step - accuracy: 0.7063 - loss: 0.8044 - val_accuracy: 0.5833 - val_loss: 1.1433\n",
      "Epoch 4/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 310ms/step - accuracy: 0.7574 - loss: 0.6923 - val_accuracy: 0.5833 - val_loss: 1.1418\n",
      "Epoch 5/5\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 310ms/step - accuracy: 0.8438 - loss: 0.5198 - val_accuracy: 0.6186 - val_loss: 1.0926\n",
      "✅ 训练完毕\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras import layers, models, Input\n",
    "# img_size = 224  # 假设你用的是这个大小\n",
    "\n",
    "# # Functional API 构建模型\n",
    "# inputs = Input(shape=(img_size, img_size, 3))\n",
    "# base_model = MobileNetV2(input_tensor=inputs, include_top=False, weights='imagenet')\n",
    "# base_model.trainable = True\n",
    "\n",
    "# x = base_model.output\n",
    "# x = layers.GlobalAveragePooling2D()(x)\n",
    "# x = layers.Dense(128, activation='relu')(x)\n",
    "# outputs = layers.Dense(train_gen.num_classes, activation='softmax')(x)\n",
    "\n",
    "# model = models.Model(inputs, outputs)\n",
    "\n",
    "# # 编译和训练\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "\n",
    "# # 保存模型\n",
    "# model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_mobilenet1.h5\")\n",
    "\n",
    "\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "# 构建模型\n",
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "base_model = MobileNetV2(input_tensor=input_tensor, include_top=False, weights=\"imagenet\")\n",
    "base_model.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "output = layers.Dense(train_gen.num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_tensor, outputs=output)\n",
    "\n",
    "# 编译 & 训练\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
    "\n",
    "# 保存为 Keras 原生格式（推荐）\n",
    "model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_model.keras\")\n",
    "print(\"✅ 训练完毕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18b2d9-61c5-439b-a812-6875d636d35f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1266f205-9b66-4e52-86f4-cecfa1322fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 训练完毕\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.save(\"/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_model.h5\")\n",
    "print(\"✅ 训练完毕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f5c95e0-9b5b-4e60-b076-1e94faea97af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade tensorflowjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33ed42c0-b0fb-493e-84c9-d2cd42031591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m🌲 Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    }
   ],
   "source": [
    "!tensorflowjs_converter \\\n",
    "  --input_format=keras \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_model.h5 \\\n",
    "  /Users/aurorazheng/documents/cat-mood-journal/cat-mood-journal/public/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d1ba38e-0ae6-4449-87fd-204882e57a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m163,968\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,422,728</span> (9.24 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,422,728\u001b[0m (9.24 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,742</span> (643.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m164,742\u001b[0m (643.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('/Users/aurorazheng/documents/cat-mood-journal/cat_dataset/cat_emotion_mobilenet.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851da202-1896-4ce2-abfc-89fcdd2742b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
